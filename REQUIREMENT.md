ä¸‹é¢ç»™ä½ ä¸€ä»½å¯æ‰§è¡Œã€å·¥ç¨‹åŒ–ã€ä¸€æ¬¡æ€§è¿ç§»çš„æ–¹æ¡ˆï¼Œç›®æ ‡æ˜¯ï¼š

å®Œå…¨è„±ç¦» Python è¿è¡Œæ—¶

Electron + C++ï¼ˆllama.cpp / llama.appï¼‰è´Ÿè´£æ¨ç†

JS ä¾§å®Œæˆ Agent / RAG / MCP

safetensors â†’ GGUF / ONNX ä¸€æ¬¡æ€§ç¦»çº¿è½¬æ¢

ä¸åšæ¦‚å¿µé“ºå«ï¼Œåªè®²ä½ è¿™ä¸ªé¡¹ç›®è¯¥æ€ä¹ˆè½åœ°è¿ç§»ã€‚

ä¸€ã€æ€»ä½“è¿ç§»åŸåˆ™ï¼ˆéå¸¸é‡è¦ï¼‰

ä½ ç°åœ¨çš„ local-llm-provider æœ¬è´¨ä¸Šæ˜¯ï¼š

Python = ç¼–æ’å±‚ + æ¨ç†å±‚


è¿ç§»åå¿…é¡»æ‹†æˆï¼š

Electron / Node.js = ç¼–æ’å±‚ï¼ˆAgent / RAG / MCP / UIï¼‰
C++ = æ¨ç†å±‚ï¼ˆLLM / VLMï¼‰


ä¸æ˜¯â€œç§»æ¤ Python ä»£ç â€
è€Œæ˜¯ æ‹†èŒè´£

äºŒã€è¿ç§»åç›®æ ‡æ¶æ„ï¼ˆæ¨èï¼‰
project-root/
â”œâ”€ electron/
â”‚  â”œâ”€ main/
â”‚  â”‚  â”œâ”€ ipc.ts
â”‚  â”‚  â”œâ”€ agent/
â”‚  â”‚  â”œâ”€ rag/
â”‚  â”‚  â”œâ”€ mcp/
â”‚  â”‚  â””â”€ tools/
â”‚  â””â”€ renderer/
â”‚
â”œâ”€ inference/
â”‚  â”œâ”€ llama/
â”‚  â”‚  â”œâ”€ llama.cpp
â”‚  â”‚  â”œâ”€ models/
â”‚  â”‚  â”‚  â”œâ”€ llm.gguf
â”‚  â”‚  â”‚  â””â”€ vlm.gguf
â”‚  â”‚  â””â”€ server
â”‚  â””â”€ onnx/
â”‚     â”œâ”€ embedding.onnx
â”‚     â””â”€ vision.onnx
â”‚
â”œâ”€ model_convert/
â”‚  â”œâ”€ to_gguf.py
â”‚  â”œâ”€ to_onnx.py
â”‚  â””â”€ config.yaml
â”‚
â””â”€ build/

ä¸‰ã€ä½ ç°æœ‰ Python é¡¹ç›®ä¸­ã€Œèƒ½ä¿ç•™ vs å¿…é¡»é‡å†™ã€
âœ… ç›´æ¥ä¿ç•™ï¼ˆé€»è¾‘å±‚ï¼‰
1ï¸âƒ£ Agent è®¾è®¡

prompt æ¨¡æ¿

tool calling åè®®

ReAct / Plan-Execute

å¤š Agent åä½œé€»è¾‘

ğŸ‘‰ é€è¡Œç¿»è¯‘æˆ TS å³å¯

2ï¸âƒ£ RAG æµç¨‹

chunk è§„åˆ™

metadata

rerank ç­–ç•¥

query â†’ topK

ğŸ‘‰ embedding / æ¨ç†ç”± C++ æä¾›

3ï¸âƒ£ MCPï¼ˆå®Œå…¨ä¸å—å½±å“ï¼‰

MCP æœ¬èº«å°±æ˜¯åè®®

Electron é‡Œç…§æ ·è·‘

ç”šè‡³æ›´é€‚åˆæ¡Œé¢ç«¯

âŒ å¿…é¡»ç§»é™¤çš„
1ï¸âƒ£ transformers / torch
2ï¸âƒ£ Python æ¨ç†é€»è¾‘
3ï¸âƒ£ Python å¤šæ¨¡æ€æ¨¡å‹
å››ã€æ¨ç†å±‚æ›¿æ¢æ–¹æ¡ˆï¼ˆæ ¸å¿ƒï¼‰
1ï¸âƒ£ LLMï¼šsafetensors â†’ GGUF â†’ llama.cpp
ä¸€æ¬¡æ€§è½¬æ¢æµç¨‹
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
python3 convert_hf_to_gguf.py \
  --outtype q6_k \
  /path/to/hf-model


è¾“å‡ºï¼š

model-q6_k.gguf


è¿™æ˜¯å”¯ä¸€æ¨èçš„ç”Ÿäº§è·¯å¾„

Electron è°ƒç”¨æ–¹å¼ï¼ˆæ¨èï¼‰
æ–¹å¼ Aï¼šllama.cpp å†…ç½® serverï¼ˆæœ€ç¨³ï¼‰
./llama-server \
  -m model.gguf \
  --port 8080 \
  --ctx-size 8192


Electronï¼š

fetch("http://127.0.0.1:8080/completion", {...})


ä¼˜ç‚¹ï¼š

å´©æºƒéš”ç¦»

å¯çƒ­é‡å¯

è¡Œä¸šä¸»æµ

æ–¹å¼ Bï¼šNode Addonï¼ˆä¸å»ºè®®é¦–ç‰ˆï¼‰

N-API åŒ…è£… llama.cpp

è°ƒè¯•æˆæœ¬é«˜

å´©æºƒç›´æ¥å¸¦èµ° Electron

2ï¸âƒ£ Embeddingï¼šsafetensors â†’ ONNX â†’ onnxruntime C++
ä¸€æ¬¡æ€§è½¬æ¢ï¼ˆç¦»çº¿ï¼‰
from transformers import AutoModel
import torch

model = AutoModel.from_pretrained("bge-small-en")
dummy = torch.randn(1, 128, dtype=torch.float32)

torch.onnx.export(
    model,
    dummy,
    "embedding.onnx",
    input_names=["input_ids"],
    output_names=["emb"]
)


Electron è°ƒç”¨ï¼š

Electron â†’ C++ onnxruntime â†’ embedding

3ï¸âƒ£ VLMï¼ˆå¯é€‰ï¼‰
ä¸»æµæ–¹æ¡ˆï¼ˆæ¨èï¼‰

Vision encoder â†’ ONNX

LLM â†’ llama.cpp

image
â†’ vision.onnx
â†’ image embedding
â†’ prompt injection
â†’ llama.cpp


llama.cpp å·²æ”¯æŒå¤šç§ vision GGUFï¼Œå¯ç›´æ¥å¤ç”¨

äº”ã€JS / Electron ä¾§ç­‰ä»·å®ç°æ˜ å°„è¡¨
Python	Electron
langchain	è‡ªå†™ Agent
FAISS	sqlite + vector
transformers.generate	HTTP / IPC
tool calling	JSON schema
asyncio	Promise / worker
å…­ã€ä¸€æ¬¡æ€§æ¨¡å‹å¤„ç†ç­–ç•¥ï¼ˆä½ è¦æ±‚çš„é‡ç‚¹ï¼‰
âœ” ä¸åœ¨ç”¨æˆ·æœºå™¨ä¸Šè½¬æ¢
âœ” ä¸ä¾èµ– Python ç¯å¢ƒ
âœ” æ¨¡å‹å³èµ„äº§
æ¨èåšæ³•
CI / Dev æœºå™¨
â”œâ”€ safetensors
â”‚  â”œâ”€ LLM â†’ GGUF
â”‚  â””â”€ Embed/Vision â†’ ONNX
â””â”€ å‘å¸ƒ
   â””â”€ Electron + gguf + onnx


ç”¨æˆ·æœºå™¨ä¸Šï¼š

åªæœ‰ exe / app

ä¸å®‰è£… python

ä¸å®‰è£… pip

ä¸å®‰è£… torch

ä¸ƒã€è¿ç§»é¡ºåºï¼ˆç…§ç€åšï¼‰
Step 1

æŠ½ç¦» Python Agent é€»è¾‘

æ”¹å†™æˆ TSï¼ˆå…ˆä¸æ¥æ¨¡å‹ï¼‰

Step 2

llama.cpp server è·‘èµ·æ¥

Electron èƒ½æ‹¿åˆ° completion

Step 3

embedding.onnx æ¥å…¥ RAG

Step 4

MCP / Tool ä¸²èµ·æ¥

Step 5

Python é¡¹ç›®åªä¿ç•™è½¬æ¢è„šæœ¬

å…«ã€ä¸€å¥è¯æ€»ç»“ï¼ˆéå¸¸å…³é”®ï¼‰

ä½ ä¸æ˜¯åœ¨â€œæŠŠ Python é¡¹ç›®å˜æˆ Electronâ€ï¼Œè€Œæ˜¯åœ¨â€œä¿ç•™æ™ºèƒ½è®¾è®¡ï¼Œæ›¿æ¢æ¨ç†å¼•æ“â€ã€‚

Agent / RAG / MCP æ˜¯èµ„äº§

Python åªæ˜¯å®éªŒå·¥å…·

Electron + C++ æ‰æ˜¯å¯äº¤ä»˜å½¢æ€

å¦‚æœä½ æ„¿æ„ï¼Œä¸‹ä¸€æ­¥æˆ‘å¯ä»¥ç›´æ¥å¸®ä½ ï¼š

æŒ‰ local-llm-provider çš„ç›®å½•ç»“æ„ç”»ä¸€ä»½ 1:1 è¿ç§»å¯¹ç…§è¡¨

ç»™ä½ ä¸€ä¸ª Electron â†” llama.cpp server çš„æœ€å°å¯è¿è¡Œæ¨¡æ¿

æˆ–è€…å†™ä¸€ä¸ª safetensors â†’ GGUF + ONNX çš„ç»Ÿä¸€ conversion pipeline